{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTdWkMnW5sjV"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow tensorflow_hub tensorflow_text\n",
        "# !pip install tensorflow tensorflow_hub tensorflow_text\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow_hub import KerasLayer\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import tensorflow_text as text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ5WGofO84f2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"nlp_ds.csv\")\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "df['Encoded_Label'] = labelencoder.fit_transform(df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcitwKVM9BmP",
        "outputId": "ee390df8-bc50-4be3-ddb9-d0014390fa3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5    571\n",
            "1    533\n",
            "0    435\n",
            "3    371\n",
            "4    299\n",
            "2    267\n",
            "Name: Encoded_Label, dtype: int64\n",
            "waterbodies-spread     571\n",
            "Littoral               533\n",
            "Deciduous-woodlands    435\n",
            "current fallow         371\n",
            "plantation/orchard     299\n",
            "Snowfall/Glacial       267\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df['Encoded_Label'].value_counts())\n",
        "print(df['Label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35PuPTAN_ZSf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['Query'], df['Encoded_Label'], test_size=0.3, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b6SGFd7JkyV"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=1000)\n",
        "X_test_sequences = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arq7dW1oAHDP"
      },
      "outputs": [],
      "source": [
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSEwQQfAMA71"
      },
      "outputs": [],
      "source": [
        "def encode_sentences(sentences):\n",
        "    preprocessed_outputs = []\n",
        "    for sentence in sentences:\n",
        "        preprocessed_text = bert_preprocess(tf.constant([sentence]))\n",
        "        output = bert_encoder(preprocessed_text)['pooled_output']\n",
        "        preprocessed_outputs.append(output)\n",
        "    return tf.concat(preprocessed_outputs, axis=0)\n",
        "\n",
        "train_sentence_embeddings = encode_sentences(X_train)\n",
        "test_sentence_embeddings = encode_sentences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHUgIl1W-lbI",
        "outputId": "a0f75238-a77f-4a6b-8aa2-7cb2487ad165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Scores: [0.77809798 0.77809798 0.76368876 0.76589595 0.76878613]\n",
            "Mean CV Score: 0.7709133614299278\n",
            "Random Forest Train Accuracy: 1.0\n",
            "Random Forest Test Accuracy: 0.784656796769852\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "train_features = train_sentence_embeddings.numpy().reshape(len(X_train), -1)\n",
        "test_features = test_sentence_embeddings.numpy().reshape(len(X_test), -1)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100,random_state=42)\n",
        "\n",
        "# Use cross-validation to evaluate the model's performance\n",
        "scores = cross_val_score(rf_classifier, train_features, y_train, cv=5)\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean CV Score:\", scores.mean())\n",
        "\n",
        "rf_classifier.fit(train_features, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = rf_classifier.score(train_features, y_train)\n",
        "test_accuracy = rf_classifier.score(test_features, y_test)\n",
        "print(\"Random Forest Train Accuracy:\", train_accuracy)\n",
        "print(\"Random Forest Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn1_ET3nBx4Z",
        "outputId": "7d8171a3-b15f-4d98-bec6-154b110dab50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Label: 3\n"
          ]
        }
      ],
      "source": [
        "new_sentence = \"Show unproductive land areas for soil improvement projects in Telangana.\"\n",
        "preprocessed_new_sentence = bert_preprocess(tf.constant([new_sentence]))\n",
        "new_sentence_embedding = bert_encoder(preprocessed_new_sentence)['pooled_output']\n",
        "new_sentence_features = new_sentence_embedding.numpy().reshape(1, -1)\n",
        "predicted_label = rf_classifier.predict(new_sentence_features)[0]\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnFdjuhdzPmK",
        "outputId": "d613fc6b-d9a1-4d7c-a0be-ace93eb05e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.784656796769852\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = rf_classifier.predict(test_features)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww4uUPAZzQO9",
        "outputId": "4b49111e-4e8e-4f22-856a-f2576da1e349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision (Macro Average): 0.7865561797810088\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "print(\"Precision (Macro Average):\", precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZviXC2yzRpw",
        "outputId": "b815f254-4500-46c4-b85a-d4ee3ddab2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall (Macro Average): 0.760818261119553\n",
            "Recall (Micro Average): 0.784656796769852\n",
            "Recall (Weighted Average): 0.784656796769852\n",
            "Recall (Per Class): [0.78518519 0.80981595 0.71621622 0.73076923 0.58762887 0.93529412]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "print(\"Recall (Macro Average):\", recall_macro)\n",
        "\n",
        "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "print(\"Recall (Micro Average):\", recall_micro)\n",
        "\n",
        "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
        "print(\"Recall (Weighted Average):\", recall_weighted)\n",
        "\n",
        "\n",
        "recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "print(\"Recall (Per Class):\", recall_per_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2NPeS1PzTuD",
        "outputId": "9e17c4b4-b287-449d-ff1c-d16132447243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score (Macro Average): 1.0\n",
            "F1 Score (Micro Average): 1.0\n",
            "F1 Score (Weighted Average): 1.0\n",
            "F1 Score (Per Class): [1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# For macro-average F1 score\n",
        "f1_macro = f1_score(y_pred, y_pred, average='macro')\n",
        "print(\"F1 Score (Macro Average):\", f1_macro)\n",
        "\n",
        "# For micro-average F1 score\n",
        "f1_micro = f1_score(y_pred, y_pred, average='micro')\n",
        "print(\"F1 Score (Micro Average):\", f1_micro)\n",
        "\n",
        "# For weighted-average F1 score\n",
        "f1_weighted = f1_score(y_pred, y_pred, average='weighted')\n",
        "print(\"F1 Score (Weighted Average):\", f1_weighted)\n",
        "\n",
        "# For calculating F1 score for each class separately, specify average=None\n",
        "f1_per_class = f1_score(y_pred, y_pred, average=None)\n",
        "print(\"F1 Score (Per Class):\", f1_per_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YxMHyHnNzWOv",
        "outputId": "9f467c7d-a6f7-4dd6-a874-d0a89badd6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[106  11   2   1   5  10]\n",
            " [ 12 132   0   7   3   9]\n",
            " [  1   2  53   1   4  13]\n",
            " [  4   2   2  76  11   9]\n",
            " [  2  10   2  12  57  14]\n",
            " [  0   3   4   0   4 159]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feVqlNPuzYQu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQYWKzuzYzd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "auc = roc_auc_score(y_true, y_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAzpsd_Mnw48"
      },
      "outputs": [],
      "source": [
        "from joblib import dump\n",
        "\n",
        "\n",
        "dump(rf_classifier, 'random_forest_model.joblib')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}